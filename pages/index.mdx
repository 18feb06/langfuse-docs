import { Card, Cards } from "nextra-theme-docs";
import { SiGithub } from "react-icons/si";
import { AiOutlineCloud } from "react-icons/ai";

<h1 style={{ marginTop: "100px" }}>`ðŸª¢ langfuse`</h1>
<span
  style={{
    display: "block",
    fontSize: "1.5rem",
    textAlign: "center",
  }}
>
  open-source tracing and feedback collection for applications built on top of
  LLMs
</span>

<div className="flex pt-5 gap-3 justify-center flex-wrap pb-20">
  <Card icon={<span>ðŸš€</span>} title="Quickstart" href="/docs/get-started" />
  <Card
    icon={<SiGithub size="24" />}
    title="Repo"
    href="https://github.com/langfuse/langfuse"
  />
  <Card
    icon={<AiOutlineCloud size="24" />}
    title="Sign in"
    href="https://cloud.langfuse.com"
  />
</div>

## Walkthrough (3 min)

_Video: Integration with Next.js `Chatbot UI` app using the Typescript SDK & demo of `langfuse` interface_

import Demo from "/components/demoVideo";

<Demo />

## `langfuse` overview

> Debug and improve your LLM-based application by logging/analyzing all user interactions, backend traces, and explicit/implicit user feedback.

**Features**

1. Data collection
   - Tracing LLM application, chain, agent via backend SDK
   - Scoring of traces based on user feedback/interaction via frontend SDKs
2. Data exploration in langfuse UI
   - Identify issues
   - Debug LLM application

**Architecture**

- Server ([cloud](/docs/cloud), [local](/docs/local), [self-hosted](/docs/self-host))
  - API server on top database
  - Web app to explore data
- [Client SDKs](/docs/sdk) to integrate with LLM application
  - Backend SDK for tracing
  - Frontend SDK for scoring of traces based on user feedback/interaction

## Why are we building `langfuse`?

1. **Situation:** LLMs introduce a _'black box'_ character to Software Engineering: outputs of LLM-based applications are unpredictable
2. **Problem:** Quality of application in production cannot be assured with testing before launching the feature
3. **Consequence:** Engineers need to embrace active learning from users in production to improve their LLM-based applications. Many teams build UX around their LLM-based application to make the imperfect outputs usable and to collect feedback from users.
4. **Need:** Holistic tracing of what happens in production, linked to user feedback and other signals to understand how to improve the application.

--> `langfuse` helps with exactly that

## Get in touch

`langfuse` is being actively developed in open source together with the community. Join our [Discord](https://discord.com/invite/DNDAarxE)! Provide feedback, report bugs, or request features via GitHub issues. If you want to chat about your use case, reach out to us via email: contact@langfuse.com
