---
description: Langfuse provides a UI to evaluate generations and traces.
---

import { Frame } from "@/components/Frame";

# Add manual score using Langfuse UI

Scores can also be added manually in the Langfuse UI.

Common use cases:

- **Collaboration**: Enable team collaboration by inviting other internal members to review a subset of traces. This human-in-the-loop evaluation can enhance the overall accuracy and reliability of your results by incorporating diverse perspectives and expertise.
- **Evaluating new product features**: This feature can be useful for new use cases where no other scores have been allocated yet.
- **Benchmarking of other scores**: Establish a human baseline score that can be used as a benchmark to compare and evaluate other scores. This can provide a clear standard of reference and enhance the objectivity of your performance evaluations.

<Frame>![Add manual score in UI](/images/docs/score-manual.gif)</Frame>

## Get in touch

Looking for a specific way to score your executions in Langfuse? Join the [Discord](/discord) and discuss your use case!
