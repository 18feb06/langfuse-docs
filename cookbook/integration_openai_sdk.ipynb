{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki7E44X5ViQB"
      },
      "source": [
        "---\n",
        "description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import\n",
        "---\n",
        "\n",
        "# OpenAI Integration (Python)\n",
        "\n",
        "If you use the OpenAI Python SDK, you can use the Langfuse **drop-in replacement** to get full logging by changing only the import.\n",
        "\n",
        "```diff\n",
        "- import openai\n",
        "+ from langfuse.openai import openai\n",
        "```\n",
        "\n",
        "_**Current limitations**_\n",
        "\n",
        "- Each generation creates a new trace in Langfuse\n",
        "- It is not possible to attach a score to the generation\n",
        "\n",
        "Use this integration if you want to get started with Langfuse super fast and mostly care about tracking costs and monitoring model inputs and outputs. This integration is work in progress and features will be added over time. Suggestions and PRs welcome!\n",
        "\n",
        "For full flexibility, consider using the fully-featured [Python SDK](/docs/integrations/sdk/python).\n",
        "\n",
        "â†’ This page is a jupyter notebook, open it on [GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/integration_openai_sdk.ipynb) or [Google Colab](http://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/integration_openai_sdk.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq04G_FSWjF-"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVOOiBtUPtOO",
        "outputId": "3f41cc29-6698-486c-d097-48cb53e64e88"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7-s-hY3PPupC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# get keys for your project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# if you do not use Langfuse Cloud\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxw57KFaWtfr"
      },
      "source": [
        "## 2. Replace import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ldSEJ0bAP4sj"
      },
      "outputs": [],
      "source": [
        "# instead of: import openai\n",
        "from langfuse.openai import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbQ7NlJuXGnx"
      },
      "source": [
        "## 3. Use SDK as usual\n",
        "\n",
        "_No changes required._\n",
        "\n",
        "Optionally:\n",
        "- Set `name` to identify a specific type of generation\n",
        "- Set `metadata` with additional information that you want to see in Langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ovnAAdbaLmD"
      },
      "source": [
        "### Chat completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c8RhokKUP9I0"
      },
      "outputs": [],
      "source": [
        "completion = openai.ChatCompletion.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7CtCNzaSrn"
      },
      "source": [
        "### Functions\n",
        "\n",
        "Simple example using Pydantic to generate the function schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJfBdHowaRgs",
        "outputId": "1269ff45-e56c-4968-8fad-832a0b854381"
      },
      "outputs": [],
      "source": [
        "%pip install pydantic==1.* --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2gA-zGk7VYYp"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class StepByStepAIResponse(BaseModel):\n",
        "    title: str\n",
        "    steps: List[str]\n",
        "schema = StepByStepAIResponse.schema() # returns a dict like JSON schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ORtNcN4-afDC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "response = openai.ChatCompletion.create(\n",
        "    name=\"test-function\",\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=[\n",
        "       {\"role\": \"user\", \"content\": \"Explain how to assemble a PC\"}\n",
        "    ],\n",
        "    functions=[\n",
        "        {\n",
        "          \"name\": \"get_answer_for_user_query\",\n",
        "          \"description\": \"Get user answer in series of steps\",\n",
        "          \"parameters\": StepByStepAIResponse.schema()\n",
        "        }\n",
        "    ],\n",
        "    function_call={\"name\": \"get_answer_for_user_query\"}\n",
        ")\n",
        "\n",
        "output = json.loads(response.choices[0][\"message\"][\"function_call\"][\"arguments\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qurrm-Ntp24O"
      },
      "source": [
        "## 4. Debug & measure in Langfuse\n",
        "\n",
        "Go to https://cloud.langfuse.com or your own instance\n",
        "\n",
        "### Dashboard\n",
        "![Dashboard](https://langfuse.com/images/docs/openai-dashboard.png)\n",
        "\n",
        "### List of generations\n",
        "![List of generations](https://langfuse.com/images/docs/openai-generation-list.png)\n",
        "\n",
        "### Chat completion\n",
        "![Chat completion](https://langfuse.com/images/docs/openai-chat.png)\n",
        "\n",
        "### Function\n",
        "![Function](https://langfuse.com/images/docs/openai-function.png)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
