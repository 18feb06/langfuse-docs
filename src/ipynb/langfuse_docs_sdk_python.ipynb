{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJXInxopjbLA"
      },
      "source": [
        "---\n",
        "description: Fully async and typed Python SDK. Uses Pydantic objects for data verification.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Python SDK\n",
        "\n",
        "[![PyPI](https://img.shields.io/pypi/v/langfuse?style=flat-square)](https://pypi.org/project/langfuse/)\n",
        "\n",
        "- [View as notebook on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_sdk_python.ipynb)\n",
        "- [Open as notebook in Google Colab](http://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_sdk_python.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL7HhNyIYNwn"
      },
      "source": [
        "This is a Python SDK used to send LLM data to Langfuse in a convenient way. It uses a worker Thread and an internal queue to manage requests to the Langfuse backend asynchronously. Hence, the SDK does not impact your latencies and also does not impcat your customers in case of exceptions.\n",
        "\n",
        "Using langchain? Use the [langchain integration](https://langfuse.com/docs/langchain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc6Uxbl3R5El"
      },
      "source": [
        "## 1. Installation\n",
        "\n",
        "The Langfuse SDKs are hosted on the pypi index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F21wZSUyKLzb",
        "outputId": "473c3b3e-ecc2-42c3-b7ce-fdd3bf97bea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langfuse in /usr/local/lib/python3.10/dist-packages (0.0.76)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from langfuse) (23.1.0)\n",
            "Requirement already satisfied: httpx<0.25.0,>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from langfuse) (0.24.1)\n",
            "Requirement already satisfied: langchain<0.0.238,>=0.0.237 in /usr/local/lib/python3.10/dist-packages (from langfuse) (0.0.237)\n",
            "Requirement already satisfied: pydantic==1.10.7 in /usr/local/lib/python3.10/dist-packages (from langfuse) (1.10.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from langfuse) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.7->langfuse) (4.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.25.0,>=0.15.4->langfuse) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx<0.25.0,>=0.15.4->langfuse) (0.17.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.25.0,>=0.15.4->langfuse) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.25.0,>=0.15.4->langfuse) (1.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.0.11,>=0.0.10 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (0.0.10)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (1.23.5)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (1.2.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.0.238,>=0.0.237->langfuse) (8.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.0->langfuse) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.0.238,>=0.0.237->langfuse) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.0.238,>=0.0.237->langfuse) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.0.238,>=0.0.237->langfuse) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.0.238,>=0.0.237->langfuse) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.0.238,>=0.0.237->langfuse) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain<0.0.238,>=0.0.237->langfuse) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain<0.0.238,>=0.0.237->langfuse) (0.9.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.15.4->langfuse) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.15.4->langfuse) (3.7.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.0.238,>=0.0.237->langfuse) (2.0.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.0.238,>=0.0.237->langfuse) (2.0.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.15.4->langfuse) (1.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain<0.0.238,>=0.0.237->langfuse) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain<0.0.238,>=0.0.237->langfuse) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAupsw1pR_6q"
      },
      "source": [
        "Initialize the client with api keys and optionally your environment. In the example we are using the cloud environment which is also the default. The Python client can modify all entities in the Langfuse API and therefore requires the secret key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iDfYwZf4KUnY"
      },
      "outputs": [],
      "source": [
        "ENV_HOST = \"https://cloud.langfuse.com\"\n",
        "ENV_SECRET_KEY = \"sk-lf-1234567890\"\n",
        "ENV_PUBLIC_KEY = \"pk-lf-1234567890\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PuPgkTU476y4"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse(ENV_PUBLIC_KEY, ENV_SECRET_KEY, ENV_HOST)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Record a simple LLM call\n",
        "Some users just want to collect the data from a single call to a LLM. To do that, you can use `generations` from the SDK and fill it with the LLM configuration and the prompt and completion."
      ],
      "metadata": {
        "id": "hoybRnFrAi9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from langfuse.model import InitialGeneration, Usage\n",
        "\n",
        "generationStartTime = datetime.now()\n",
        "\n",
        "# call to an LLM API\n",
        "\n",
        "langfuse.generation(InitialGeneration(\n",
        "    name=\"summary-generation\",\n",
        "    startTime=generationStartTime,\n",
        "    endTime=datetime.now(),\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    modelParameters={\"maxTokens\": \"1000\", \"temperature\": \"0.9\"},\n",
        "    prompt=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...\"}],\n",
        "    completion=\"The Q3 OKRs contain goals for multiple teams...\",\n",
        "    usage=Usage(promptTokens=50, completionTokens = 49),\n",
        "    metadata={\"interface\": \"whatsapp\"}\n",
        "))"
      ],
      "metadata": {
        "id": "JtMxqNh0Azn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1028db-544e-41bc-b80b-ec1daa48063d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulGenerationClient at 0x7e161d5d4a60>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT4uaBm4SLvw"
      },
      "source": [
        "## 3. Record a more complex applications\n",
        "```\n",
        "TRACE\n",
        "|\n",
        "|-- SPAN: Retrieval [retrieve data from vector db]\n",
        "|   |\n",
        "|   |-- GENERATION: Vector DB Query Creation [hallucinate a query]\n",
        "|   |\n",
        "|   |-- SPAN: Data Fetching [query the db]\n",
        "|   |\n",
        "|   |-- GENERATION: Data Summary Creation [summarize the results]\n",
        "|\n",
        "|-- GENERATION: Output Generation [generate the final user output]\n",
        "```\n",
        "Some users have complex LLM features which contain vector database searches, multiple LLM calls and more as depicted above. The Langfuse SDK and UI is designed to support chaining and nesting to support these usecases. Understanding a small number of terms makes it easy to integratw with Langfuse.\n",
        "\n",
        "#### Traces\n",
        "A `trace` represents a single execution of a LLM feature. It is a container for all succeeding objects.\n",
        "#### Observations\n",
        "Each trace can contain multiple `observations` to record individual steps of an execution. There are different types of `observations`.\n",
        "  - `Events` are the basic building block. They are used to track discrete events in a trace.\n",
        "  - `Spans` can be used to record steps from a chain like fetching data from a vector databse. You are able to record inputs, outputs and more.\n",
        "  - `Generations` are a specific type of `spans` which are used to record generations of an AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langfuse.model import CreateTrace, CreateSpan, CreateGeneration, CreateEvent\n",
        "\n",
        "trace = langfuse.trace(CreateTrace(name = \"llm-feature\"))\n",
        "retrieval = trace.span(CreateSpan(name = \"retrieval\"))\n",
        "retrieval.generation(CreateSpan(name = \"query-creation\"))\n",
        "retrieval.span(CreateGeneration(name = \"vector-db-search\"))\n",
        "retrieval.generation(CreateEvent(name = \"db-summary\"))\n",
        "generation = trace.generation(CreateGeneration(name = \"user-output\"))"
      ],
      "metadata": {
        "id": "N-1Ta3wVGGxq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Object details"
      ],
      "metadata": {
        "id": "3cNeYgeuF6u0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GjVFk7N9jZr"
      },
      "source": [
        "### Traces\n",
        "\n",
        "Traces are the top-level entity in the Langfuse API. They represent an execution flow in a LLM application usually triggered by an external event.\n",
        "\n",
        "Traces can be created and updated.\n",
        "\n",
        "`trace.create()` takes the following parameters:\n",
        "\n",
        "- `name` (optional): identifier of the trace. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the trace. Can be any JSON object.\n",
        "- `id` (optional): The id of the trace can be set, otherwise a random one is generated. Useful for linking traces to external systems or when grouping multiple runs into a single trace (e.g. messages in a chat thread).\n",
        "- `userId` (optional): the id of the user who triggered the execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z9Kxxjp004WD"
      },
      "outputs": [],
      "source": [
        "from langfuse.model import CreateTrace\n",
        "\n",
        "trace = langfuse.trace(CreateTrace(\n",
        "    name = \"docs-retrieval\",\n",
        "    userId = \"user__935d7d1d-8625-4ef4-8651-544613e7bd22\",\n",
        "    metadata = {\n",
        "        \"env\": \"production\",\n",
        "        \"email\": \"user@langfuse.com\",\n",
        "    }\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtWxwt3H90qF"
      },
      "source": [
        "### Span\n",
        "\n",
        "Spans represent durations of units of work in a trace. We generated convenient SDK functions for generic spans as well as LLM spans.\n",
        "\n",
        "Span creation takes the following parameters:\n",
        "\n",
        "- `startTime` (optional): the time at which the span started. If no startTime is provided, the current time will be used.\n",
        "- `endTime` (optional): the time at which the span ended. Can also be set using `span.update()`.\n",
        "- `name` (optional): identifier of the span. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the span. Can be any JSON object. Can also be set or updated using `span.update()`.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "- `input` (optional): the input to the span. Can be any JSON object.\n",
        "- `output` (optional): the output to the span. Can be any JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "otJQPNC198Ti"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from langfuse.model import CreateSpan, UpdateSpan\n",
        "\n",
        "retrievalStartTime = datetime.now()\n",
        "\n",
        "# retrieveDocs = retrieveDoc()\n",
        "# ...\n",
        "\n",
        "span = trace.span(CreateSpan(\n",
        "        name=\"embedding-search\",\n",
        "        startTime=retrievalStartTime,\n",
        "        endTime=datetime.now(),\n",
        "        metadata={\"database\": \"pinecone\"},\n",
        "        input = {'query': 'This document entails the OKR goals for ACME'},\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spans can be updated once your function completes for example record outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "NZ7qCdGAOYvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "span = span.update(UpdateSpan(\n",
        "        output = {\"response\": \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"}\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "r7I_JwbxOd82"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNPQH8Nz-duo"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Generations are used to log generations of AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI.\n",
        "\n",
        "`langfuse.generation()` take the following parameters:\n",
        "\n",
        "- `startTime` (optional): the time at which the generation started.\n",
        "- `endTime` (optional): the time at which the generation ended.\n",
        "- `name` (optional): identifier of the generation. Useful for sorting/filtering in the UI.\n",
        "- `model` (optional): the name of the model used for the generation\n",
        "- `modelParameters` (optional): the parameters of the model used for the generation; can be any key-value pairs\n",
        "- `prompt` (optional): the prompt used for the generation; can be any string or JSON object (recommended for chat models or other models that use structured input)\n",
        "- `completion` (optional): the completion generated by the model\n",
        "- `usage` (optional): the usage of the model during the generation; takes two optional key-value pairs: `promptTokens` and `completionTokens`\n",
        "- `metadata` (optional): additional metadata of the generation. Can be any JSON object.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nJfTbXvNQ6iD"
      },
      "outputs": [],
      "source": [
        "from langfuse.model import CreateGeneration, Usage, UpdateGeneration\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "generationStartTime = datetime.now()\n",
        "\n",
        "# chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "# ...\n",
        "\n",
        "generation = trace.generation(CreateGeneration(\n",
        "    name=\"summary-generation\",\n",
        "    startTime=generationStartTime,\n",
        "    endTime=datetime.now(),\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    modelParameters={\"maxTokens\": \"1000\", \"temperature\": \"0.9\"},\n",
        "    prompt=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...\"}],\n",
        "    metadata={\"interface\": \"whatsapp\"}\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generations can be updated once your LLM function completes for example record outputs."
      ],
      "metadata": {
        "id": "cYb2kvAMOqHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation.update(UpdateGeneration(\n",
        "    completion=\"The Q3 OKRs contain goals for multiple teams...\",\n",
        "    usage=Usage(promptTokens=50, completionTokens = 49),\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQccImMOOz8F",
        "outputId": "775ec5fb-2125-4898-a3fa-67bd6f766d4c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulGenerationClient at 0x7e161d5d6470>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfzAYslz9Aks"
      },
      "source": [
        "### Events\n",
        "\n",
        "Events are used to track discrete events in a trace.\n",
        "\n",
        "- `startTime`: the time at which the event started.\n",
        "- `name` (optional): identifier of the event. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the event. JSON object.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "- `input` (optional): the input to the event. Can be any JSON object.\n",
        "- `output` (optional): the output to the event. Can be any JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tuSjykFW9Iw1"
      },
      "outputs": [],
      "source": [
        "from langfuse.model import CreateEvent\n",
        "from datetime import datetime\n",
        "\n",
        "event = span.event(CreateEvent(\n",
        "        name=\"chat-docs-retrieval\",\n",
        "        startTime=datetime.now(),\n",
        "        metadata={\"key\": \"value\"},\n",
        "        input = {\"key\": \"value\"},\n",
        "        output = {\"key\": \"value\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4h-gogK-YLh"
      },
      "source": [
        "`span.update()` take the following parameters:\n",
        "\n",
        "- `spanId`: the id of the span to update\n",
        "- `endTime` (optional): the time at which the span ended\n",
        "- `metadata` (optional): merges with existing metadata of the span. Can be any JSON object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABnZymiSej8"
      },
      "source": [
        "## 3. Collect (user) feedback\n",
        "\n",
        "Scores are used to evaluate single executions/traces. They can be supplied internally through our UI or via the SDK. If the score relates to a specific step of the trace, the score can optionally also be attached to the observation to enable evaluating it specifically.\n",
        "\n",
        "- `traceId`: the id of the trace to which the score should be attached, automatically set when using trace.score() instead of langfuse.score()\n",
        "- `name`: identifier of the score, string\n",
        "- `value`: the value of the score; float; optional: scale it to e.g. 0..1 to make it comparable to other scores\n",
        "- `comment` (optional): additional context/explanation of the score\n",
        "- `observationId` (optional): the id of the observation to which the score should be attached, automatically set when using span.score()/event.score()/generation.score() instead of langfuse.score()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj19Zby3SfT9",
        "outputId": "cae1407e-b831-40c4-b9ad-ee06b3e6e3b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulClient at 0x7e161d5d4d00>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from langfuse.model import CreateScore\n",
        "\n",
        "trace.score(CreateScore(\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Technical considerations"
      ],
      "metadata": {
        "id": "9gHGMs9QINYG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q5aljyIoU42"
      },
      "source": [
        "## Serverless environments\n",
        "\n",
        "The Langfuse SDK executes network requests in the background on a separate thread for better performance of your application. This can lead to lost events in short lived environments like NextJs cloud functions or AWS Lambda functions when the python process is terminated before the SDK sent the event to our backend.\n",
        "\n",
        "To avoid this, ensure that the `langfuse.flush()` function is called after each request. This method is waiting for all tasks to have completed, hence it is blocking.[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5jpWEosnINa4"
      },
      "outputs": [],
      "source": [
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastAPI\n",
        "Some of our users are using FastAPI, hence we have a short example, of how to use it there. [Here](https://github.com/langfuse/fastapi_demo) is a Git Repo with all the details.\n"
      ],
      "metadata": {
        "id": "-GcLUd6_JXWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install fastapi"
      ],
      "metadata": {
        "id": "V9WHoZAmKEgO",
        "outputId": "5191e436-1c9d-4398-de78-e6408d8d7571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.101.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.7)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.7.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of how to initialise FastAPI and register the `langfuse.flush()` method to run at shutdown.\n",
        "With this, your Python environment will only terminate, once Langfuse received all the events."
      ],
      "metadata": {
        "id": "COd5Q_67KMqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import asynccontextmanager\n",
        "from fastapi import FastAPI, Query, BackgroundTasks\n",
        "from langfuse.model import InitialGeneration\n",
        "\n",
        "\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    # Operation on startup\n",
        "\n",
        "    yield  # wait until shutdown\n",
        "\n",
        "    # Flush all events to be sent to Langfuse on shutdown. This operation is blocking.\n",
        "    langfuse.flush()\n",
        "\n",
        "\n",
        "app = FastAPI(lifespan=lifespan)"
      ],
      "metadata": {
        "id": "Nu2ug2AoJ8d8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langfuse = Langfuse(\"pk-lf-1234567890\", \"sk-lf-1234567890\", \"http://localhost:3000\")\n",
        "\n",
        "@app.get(\"/generate/\",tags=[\"APIs\"])\n",
        "async def campaign(prompt: str = Query(..., max_length=20)):\n",
        "  # call to a LLM\n",
        "  generation = langfuse.generation(\n",
        "      InitialGeneration(name=\"llm-feature\", metadata=\"test\", prompt=prompt)\n",
        "  )\n",
        "  return True"
      ],
      "metadata": {
        "id": "L91osyXKKZIn"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}