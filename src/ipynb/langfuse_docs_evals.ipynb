{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model based evals with Langfuse\n",
        "\n",
        "- [View as notebook on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_evals.ipynb)\n",
        "- [Open as notebook in Google Colab](http://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/src/ipynb/langfuse_docs_evals.ipynb)\n",
        "\n",
        "Evaluating the quality of LLM outputs is most of the time manual and hence very time consuming, as reading large amounts of text takes a lot of time. This Cookbook shows, how this can be automated using data which was captured in [Langfuse](http://langfuse.com/) already.\n",
        "\n",
        "This cookbook can be easily adjusted to use any eval library.\n",
        "\n",
        "In this example we will:\n",
        "1. Fetch `Generations` stored in Langfuse\n",
        "2. Evaluate these `Generations` using Langchain\n",
        "3. Submit results back to Langfuse\n",
        "\n",
        "\n",
        "----\n",
        "Not using Langfuse yet? Get started by capturing LLM events: [Python](https://langfuse.com/docs/integrations/sdk/python), [TS/JS](https://langfuse.com/docs/integrations/sdk/typescript)"
      ],
      "metadata": {
        "id": "SWL354n0DECo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First we need to install `langfuse` and `langchain` set the environment variables. Afterwards, we initialise the SDK, more information can be found [here](https://langfuse.com/docs/integrations/sdk/python#1-installation)."
      ],
      "metadata": {
        "id": "WbfTYaTkEu3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langfuse langchain openai"
      ],
      "metadata": {
        "id": "Qclwxd9LRPAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LF_PK'] = \"pk-lf-...\"\n",
        "os.environ['LF_SK'] = \"sk-lf-...\"\n",
        "os.environ['EVAL_MODEL'] = \"text-davinci-003\"\n",
        "os.environ['HOST'] = \"https://cloud.langfuse.com\"\n",
        "os.environ[\"OPENAI_API_KEY\"]='sk-...'\n",
        "\n",
        "EVAL_TYPES={\n",
        "    \"conciseness\": True,\n",
        "    \"relevance\": True,\n",
        "    \"coherence\": True,\n",
        "    \"harmfulness\": True,\n",
        "    \"maliciousness\": True,\n",
        "    \"helpfulness\": True,\n",
        "    \"controversiality\": True,\n",
        "    \"misogyny\": True,\n",
        "    \"criminality\": True,\n",
        "    \"insensitivity\": True\n",
        "}\n"
      ],
      "metadata": {
        "id": "CQhmQQpLRa1K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8viV4KT5RMjA"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse(os.environ.get(\"LF_PK\"), os.environ.get(\"LF_SK\"), os.environ.get(\"HOST\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching data\n",
        "\n",
        "Below, we load all `Generations` from Langfuse by name. The name can be submitted via our SDKs when capturing LLM calls. See [docs](https://langfuse.com/docs/integrations/sdk/python#generation)"
      ],
      "metadata": {
        "id": "bjMZ1VLhF2Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_all_pages(name, limit=50):\n",
        "    page = 1\n",
        "    all_data = []\n",
        "\n",
        "    while True:\n",
        "        response = langfuse.get_generations(name=name, limit=limit, page=page)\n",
        "        if not response.data:\n",
        "            break\n",
        "\n",
        "        all_data.extend(response.data)\n",
        "        page += 1\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "3r3jOEX0RvXi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generations = fetch_all_pages(name=\"OpenAI\")\n",
        "print(len(generations))"
      ],
      "metadata": {
        "id": "cAnLShvjBDBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation + submission to Langfuse\n",
        "\n",
        "In this case we use the `conciseness` evaluation by Langchain to evaluate all the `Generations`. See the [docs](https://python.langchain.com/docs/guides/evaluation/) for Langfuse evaluations.\n",
        "Each score is provided to Langchain via the [scoring API](https://langfuse.com/docs/scores).\n",
        "\n",
        "After submitting all scores, they can be viewed in Langfuse.\n",
        "\n",
        "![Image of Trace](https://langfuse.com/images/docs/trace.jpg)\n"
      ],
      "metadata": {
        "id": "hYM6UG_dGbb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation import load_evaluator, EvaluatorType\n",
        "from langchain import PromptTemplate, OpenAI, LLMChain\n",
        "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n",
        "\n",
        "def get_evaluator_for_key(key: str):\n",
        "  llm = OpenAI(temperature=0, model=os.environ.get('EVAL_MODEL'))\n",
        "  if key == 'hallucination':\n",
        "    criteria = {\n",
        "        \"hallucination\": (\n",
        "            \"Does this submission contain information\"\n",
        "            \" not present in the input or reference?\"\n",
        "        ),\n",
        "    }\n",
        "    return LabeledCriteriaEvalChain.from_llm(\n",
        "        llm=llm,\n",
        "        criteria=criteria,\n",
        "    )\n",
        "  elif key == \"correctness\":\n",
        "    evaluator = LabeledCriteriaEvalChain.from_llm(\n",
        "      llm=llm,\n",
        "      criteria='correctness',\n",
        "   )\n",
        "  else:\n",
        "      return load_evaluator(\"criteria\", criteria=key, llm=llm)\n"
      ],
      "metadata": {
        "id": "7NijTmslvyK8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langfuse.model import InitialScore\n",
        "\n",
        "\n",
        "\n",
        "for generation in generations:\n",
        "  criteria = [key for key, value in EVAL_TYPES.items() if value]\n",
        "\n",
        "  for criterion in criteria:\n",
        "    print(criterion)\n",
        "    eval_result = get_evaluator_for_key(criterion).evaluate_strings(\n",
        "        prediction=generation.completion,\n",
        "        input=generation.prompt,\n",
        "    )\n",
        "    print(eval_result)\n",
        "\n",
        "    langfuse.score(InitialScore(name='conciseness', traceId=generation.trace_id, observationId=generation.id, value=eval_result[\"score\"], comment=eval_result['reasoning']))\n",
        "\n",
        "langfuse.flush()\n"
      ],
      "metadata": {
        "id": "qMa2OEtqvyGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}