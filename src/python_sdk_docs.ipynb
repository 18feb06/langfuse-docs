{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBspBzuRk9C"
      },
      "source": [
        "# Python SDK\n",
        "\n",
        "- [View as notebook on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/src/python_sdk_docs.ipynb)\n",
        "- [Open as notebook in Google Colab](http://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/src/python_sdk_docs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc6Uxbl3R5El"
      },
      "source": [
        "## 1. Initializing the client\n",
        "\n",
        "The langfuse SDKs are hosted on the pypi index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F21wZSUyKLzb",
        "outputId": "ea8eda3a-1358-4fec-c943-f6e714beaebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langfuse==0.0.39\n",
            "  Downloading langfuse-0.0.39-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from langfuse==0.0.39) (23.1.0)\n",
            "Requirement already satisfied: httpx<0.25.0,>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from langfuse==0.0.39) (0.24.1)\n",
            "Requirement already satisfied: pydantic==1.10.7 in /usr/local/lib/python3.10/dist-packages (from langfuse==0.0.39) (1.10.7)\n",
            "Requirement already satisfied: pytest<8.0.0,>=7.4.0 in /usr/local/lib/python3.10/dist-packages (from langfuse==0.0.39) (7.4.0)\n",
            "Requirement already satisfied: pytest-asyncio<0.22.0,>=0.21.1 in /usr/local/lib/python3.10/dist-packages (from langfuse==0.0.39) (0.21.1)\n",
            "Requirement already satisfied: pytest-timeout<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from langfuse==0.0.39) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from langfuse==0.0.39) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.7->langfuse==0.0.39) (4.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.25.0,>=0.15.4->langfuse==0.0.39) (2023.5.7)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx<0.25.0,>=0.15.4->langfuse==0.0.39) (0.17.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.25.0,>=0.15.4->langfuse==0.0.39) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.25.0,>=0.15.4->langfuse==0.0.39) (1.3.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.4.0->langfuse==0.0.39) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.4.0->langfuse==0.0.39) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.4.0->langfuse==0.0.39) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.4.0->langfuse==0.0.39) (1.1.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest<8.0.0,>=7.4.0->langfuse==0.0.39) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.0->langfuse==0.0.39) (1.16.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.15.4->langfuse==0.0.39) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.15.4->langfuse==0.0.39) (3.7.1)\n",
            "Installing collected packages: langfuse\n",
            "  Attempting uninstall: langfuse\n",
            "    Found existing installation: langfuse 0.0.38\n",
            "    Uninstalling langfuse-0.0.38:\n",
            "      Successfully uninstalled langfuse-0.0.38\n",
            "Successfully installed langfuse-0.0.39\n"
          ]
        }
      ],
      "source": [
        "%pip install langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAupsw1pR_6q"
      },
      "source": [
        "Initialize the client with api keys and optionally your environment. In the example we are using the cloud environment which is also the default. The Python client can modify all entities in the Langfuse API and therefore requires the secret key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iDfYwZf4KUnY"
      },
      "outputs": [],
      "source": [
        "ENV_HOST = \"https://cloud.langfuse.com\"\n",
        "ENV_SECRET_KEY = \"sk-lf-1234567890\"\n",
        "ENV_PUBLIC_KEY = \"pk-lf-1234567890\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PuPgkTU476y4"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "langfuse = Langfuse(ENV_PUBLIC_KEY, ENV_SECRET_KEY, ENV_HOST)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flushing\n",
        "\n",
        "The langfuse client is built asynchronous to not add latency. Only when calling the flush function, the network requests to the langfuse backend will be executed.\n",
        "\n",
        "Langfuse offers two different fush functions. `async_flush` returns a coroutine and hence can be used in async contexts such as this Notebook. `flush` is a synchronous function and takes care of asynchronous code in the background and is blocking."
      ],
      "metadata": {
        "id": "-q5aljyIoU42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# result = await client.flush()\n",
        "# returns a result and executes a coroutine in the background\n",
        "\n",
        "result = await langfuse.async_flush() # returns a coroutine\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jpWEosnINa4",
        "outputId": "527190ba-c1a9-49d6-bf26-6fd22b995a3a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'status': 'success'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT4uaBm4SLvw"
      },
      "source": [
        "## 2. Trace execution of backend\n",
        "\n",
        "- Each backend execution is logged with a single `trace`.\n",
        "- Each trace can contain multiple `observations` to log the individual steps of the execution.\n",
        "  - Observations can be nested.\n",
        "  - Observations can be of different types\n",
        "    - `Events` are the basic building block. They are used to track discrete events in a trace.\n",
        "    - `Spans` represent durations of units of work in a trace.\n",
        "    - `Generations` are spans which are used to log generations of AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GjVFk7N9jZr"
      },
      "source": [
        "### Traces\n",
        "\n",
        "Traces are the top-level entity in the Langfuse API. They represent an execution flow in a LLM application usually triggered by an external event.\n",
        "\n",
        "Traces can be created and updated.\n",
        "\n",
        "`trace.create()` takes the following parameters:\n",
        "\n",
        "- `name` (optional): identifier of the trace. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the trace. Can be any JSON object.\n",
        "- `externalId` (optional): the id of the execution in the external system. Useful for linking traces to external systems. Frequently used to create scores without having access to the langfuse `traceId`.\n",
        "- `userId` (optional): the id of the user who triggered the execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z9Kxxjp004WD"
      },
      "outputs": [],
      "source": [
        "from langfuse.api.model import CreateTrace\n",
        "\n",
        "trace = langfuse.trace(CreateTrace(\n",
        "    name = \"docs-retrieval\",\n",
        "    userId = \"user__935d7d1d-8625-4ef4-8651-544613e7bd22\",\n",
        "    metadata = {\n",
        "        \"env\": \"production\",\n",
        "        \"email\": \"user@langfuse.com\",\n",
        "    }\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtWxwt3H90qF"
      },
      "source": [
        "### Span\n",
        "\n",
        "Spans represent durations of units of work in a trace. We generated convenient SDK functions for generic spans as well as LLM spans.\n",
        "\n",
        "`span.create()` take the following parameters:\n",
        "\n",
        "- `startTime` (optional): the time at which the span started. If no startTime is provided, the current time will be used.\n",
        "- `endTime` (optional): the time at which the span ended. Can also be set using `span.update()`.\n",
        "- `name` (optional): identifier of the span. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the span. Can be any JSON object. Can also be set or updated using `span.update()`.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "- `input` (optional): the input to the span. Can be any JSON object.\n",
        "- `output` (optional): the output to the span. Can be any JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "otJQPNC198Ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d688617-a3f3-4684-dada-81f3b412400b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "span body id=None trace_id_type=None name='embedding-search' start_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 533465) end_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 533560) metadata={'database': 'pinecone'} input={'query': 'This document entails the OKR goals for ACME'} output={'response': \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"} level=None status_message=None <class 'langfuse.api.model.CreateSpan'> {'query': 'This document entails the OKR goals for ACME'} {'response': \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"}\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "from langfuse.api.model import CreateSpan\n",
        "\n",
        "retrievalStartTime = datetime.datetime.now()\n",
        "\n",
        "# retrieveDocs = retrieveDoc()\n",
        "# ...\n",
        "\n",
        "span = trace.span(CreateSpan(\n",
        "        name=\"embedding-search\",\n",
        "        startTime=retrievalStartTime,\n",
        "        endTime=datetime.datetime.now(),\n",
        "        metadata={\"database\": \"pinecone\"},\n",
        "        input = {'query': 'This document entails the OKR goals for ACME'},\n",
        "        output = {\"response\": \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNPQH8Nz-duo"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Generations are used to log generations of AI model. They contain additional metadata about the model and the prompt/completion and are specifically rendered in the langfuse UI.\n",
        "\n",
        "`generation.log()` take the following parameters:\n",
        "\n",
        "- `startTime` (optional): the time at which the generation started.\n",
        "- `endTime` (optional): the time at which the generation ended.\n",
        "- `name` (optional): identifier of the generation. Useful for sorting/filtering in the UI.\n",
        "- `model` (optional): the name of the model used for the generation\n",
        "- `modelParameters` (optional): the parameters of the model used for the generation; can be any key-value pairs\n",
        "- `prompt` (optional): the prompt used for the generation; can be any string or JSON object (recommended for chat models or other models that use structured input)\n",
        "- `completion` (optional): the completion generated by the model\n",
        "- `usage` (optional): the usage of the model during the generation; takes two optional key-value pairs: `promptTokens` and `completionTokens`\n",
        "- `metadata` (optional): additional metadata of the generation. Can be any JSON object.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nJfTbXvNQ6iD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389f4f55-91b8-4554-8b37-0ebff89f05ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generation:  id=None trace_id_type=None name='summary-generation' start_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 559214) end_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 560850) completion_start_time=None model='gpt-3.5-turbo' model_parameters={'maxTokens': '1000', 'temperature': '0.9'} prompt=[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...'}] metadata={'interface': 'whatsapp'} completion='The Q3 OKRs contain goals for multiple teams...' usage=Usage(prompt_tokens=50, completion_tokens=49, total_tokens=None) level=None status_message=None ff1214d9-349a-4461-b491-cedebc31319d\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulGenerationClient at 0x7dfb47abf9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from langfuse.api.model import CreateGeneration, Usage\n",
        "import datetime\n",
        "\n",
        "generationStartTime = datetime.datetime.now()\n",
        "\n",
        "# chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "# ...\n",
        "\n",
        "trace.generation(CreateGeneration(\n",
        "    name=\"summary-generation\",\n",
        "    startTime=generationStartTime,\n",
        "    endTime=datetime.datetime.now(),\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    modelParameters={\"maxTokens\": \"1000\", \"temperature\": \"0.9\"},\n",
        "    prompt=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...\"}],\n",
        "    completion=\"The Q3 OKRs contain goals for multiple teams...\",\n",
        "    usage=Usage(promptTokens=50, completionTokens = 49),\n",
        "    metadata={\"interface\": \"whatsapp\"}\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfzAYslz9Aks"
      },
      "source": [
        "### Events\n",
        "\n",
        "Events are used to track discrete events in a trace.\n",
        "\n",
        "- `startTime`: the time at which the event started.\n",
        "- `name` (optional): identifier of the event. Useful for sorting/filtering in the UI.\n",
        "- `metadata` (optional): additional metadata of the event. JSON object.\n",
        "- `level` (optional): the level of the event. Can be `DEBUG`, `DEFAULT`, `WARNING` or `ERROR`. Used for sorting/filtering of traces with elevated error levels and for highlighting in the UI.\n",
        "- `statusMessage` (optional): the status message of the event. Additional field for context of the event. E.g. the error message of an error event.\n",
        "- `input` (optional): the input to the event. Can be any JSON object.\n",
        "- `output` (optional): the output to the event. Can be any JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tuSjykFW9Iw1"
      },
      "outputs": [],
      "source": [
        "from langfuse.api.model import CreateEvent\n",
        "import datetime\n",
        "\n",
        "event = span.event(CreateEvent(\n",
        "        name=\"chat-docs-retrieval\",\n",
        "        startTime=datetime.datetime.now(),\n",
        "        metadata={\"key\": \"value\"},\n",
        "        input = {\"key\": \"value\"},\n",
        "        output = {\"key\": \"value\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4h-gogK-YLh"
      },
      "source": [
        "`span.update()` take the following parameters:\n",
        "\n",
        "- `spanId`: the id of the span to update\n",
        "- `endTime` (optional): the time at which the span ended\n",
        "- `metadata` (optional): merges with existing metadata of the span. Can be any JSON object."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nesting of observations\n",
        "\n",
        "Nesting of observations is helpful to structure the trace in a hierarchical way. This is especially helpful for complex chains and agents.\n",
        "\n",
        "```\n",
        "Simple example\n",
        "- trace: chat-app-session\n",
        "  - span: chat-interaction\n",
        "    - event: get-user-profile\n",
        "    - generation: chat-completion\n",
        "```"
      ],
      "metadata": {
        "id": "WDoIw_Wj7r3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trace = langfuse.trace(CreateTrace(name = \"chat-app-session\"))\n",
        "span = trace.span(CreateSpan(name = \"chat-interaction\"))\n",
        "event = span.event(CreateEvent(name = \"get-user-profile\"))\n",
        "generation = span.generation(CreateGeneration(name = \"chat-completion\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0qJYfnw8BDb",
        "outputId": "3935e0d1-eb9e-4781-ea3c-18b8c7c96efd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "span body id=None trace_id_type=None name='chat-interaction' start_time=None end_time=None metadata=None input=None output=None level=None status_message=None <class 'langfuse.api.model.CreateSpan'> None None\n",
            "generation:  id=None trace_id_type=None name='chat-completion' start_time=None end_time=None completion_start_time=None model=None model_parameters=None prompt=None metadata=None completion=None usage=None level=None status_message=None ed1a5f59-72f2-43f9-8675-00610248d12c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABnZymiSej8"
      },
      "source": [
        "## 3. Collect scores\n",
        "\n",
        "Scores are used to evaluate executions/traces. They are always attached to a single trace. If the score relates to a specific step of the trace, the score can optionally also be atatched to the observation to enable evaluating it specifically.\n",
        "\n",
        "- `traceId`: the id of the trace to which the score should be attached\n",
        "- `name`: identifier of the score, string\n",
        "- `value`: the value of the score; float; optional: scale it to e.g. 0..1 to make it comparable to other scores\n",
        "- `traceIdType` (optional): the type of the traceId. Can be `LANGFUSE` (default) or `EXTERNAL`. If `EXTERNAL` is used, the score will be attached to the trace with the given externalId.\n",
        "- `comment` (optional): additional context/explanation of the score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mj19Zby3SfT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e84e5c-fc35-4d8f-b07d-6b37e3989798"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulClient at 0x7dfb2a5bbc10>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from langfuse.api.model import CreateScore\n",
        "\n",
        "\n",
        "trace.score(CreateScore(\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = await langfuse.async_flush()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEh4aFFfIK5I",
        "outputId": "017ced5a-c8c9-460c-d52e-7e86eb6e30a6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running task:  <function Langfuse.trace.<locals>.<lambda> at 0x7dfb2a56f370> () {}\n",
            "parent id='clka6txk40003mg08yr9br90i' timestamp=datetime.datetime(2023, 7, 19, 20, 40, 28, 707000, tzinfo=datetime.timezone.utc) external_id=None name='docs-retrieval' user_id='user__935d7d1d-8625-4ef4-8651-544613e7bd22' metadata={'env': 'production', 'email': 'user@langfuse.com'}\n",
            "new_body id='fa1a6271-4fc8-4b95-b9ee-b750c05ed425' trace_id_type=None name='embedding-search' start_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 533465) end_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 533560) metadata={'database': 'pinecone'} input={'query': 'This document entails the OKR goals for ACME'} output={'response': \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"} level=None status_message=None trace_id='clka6txk40003mg08yr9br90i'\n",
            "submitting span:  id='fa1a6271-4fc8-4b95-b9ee-b750c05ed425' trace_id='clka6txk40003mg08yr9br90i' trace_id_type=None name='embedding-search' start_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 533465) end_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 533560) metadata={'database': 'pinecone'} input={'query': 'This document entails the OKR goals for ACME'} output={'response': \"[{'name': 'OKR Engineering', 'content': 'The engineering department defined the following OKR goals...'},{'name': 'OKR Marketing', 'content': 'The marketing department defined the following OKR goals...'}]\"} level=None status_message=None parent_observation_id=None\n",
            "submitting generation:  id='ebc1170a-14fc-48a8-8a39-6533a57f62da' trace_id='clka6txk40003mg08yr9br90i' trace_id_type=None name='summary-generation' start_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 559214) end_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 560850) completion_start_time=None model='gpt-3.5-turbo' model_parameters={'maxTokens': '1000', 'temperature': '0.9'} prompt=[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Please generate a summary of the following documents \\nThe engineering department defined the following OKR goals...\\nThe marketing department defined the following OKR goals...'}] metadata={'interface': 'whatsapp'} completion='The Q3 OKRs contain goals for multiple teams...' usage=LlmUsage(prompt_tokens=50, completion_tokens=49, total_tokens=None) level=None status_message=None parent_observation_id=None\n",
            "submitting score:  id=None trace_id='clka6txk40003mg08yr9br90i' trace_id_type=None name='user-explicit-feedback' value=1 observation_id=None comment='I like how personalized the response is'\n",
            "submitting event:  id='9ffafe1a-2d27-4f15-a82f-77e8007a7c68' trace_id='clka6txk40003mg08yr9br90i' trace_id_type=None name='chat-docs-retrieval' start_time=datetime.datetime(2023, 7, 19, 20, 40, 27, 545172) metadata={'key': 'value'} input={'key': 'value'} output={'key': 'value'} level=None status_message=None parent_observation_id='fa1a6271-4fc8-4b95-b9ee-b750c05ed425'\n",
            "{'status': 'success'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgtTX-Vd_Ac-"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "If you encounter any issue, we are happy to help on [Discord](https://discord.gg/7NXusRtqYU) or shoot us an email: help@langfuse.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}